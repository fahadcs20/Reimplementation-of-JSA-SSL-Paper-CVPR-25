{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Joint Slot Attention for Sound Source Localization\n\nImplementation of \"Improving Sound Source Localization with Joint Slot Attention on Image and Audio\"\n\nThis notebook implements the nearly full paper architecture(with dummy dataset) including:\n- Joint Slot Attention mechanism\n- Cross-modal attention matching\n- All loss functions (contrastive, matching, divergence, reconstruction)\n- False negative mitigation\n- IQR inference refinement","metadata":{}},{"cell_type":"code","source":"# Cell 1: Install required packages\n!pip install torch torchvision torchaudio librosa soundfile scikit-learn matplotlib seaborn -q","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-06T05:40:25.718276Z","iopub.execute_input":"2026-02-06T05:40:25.718615Z","iopub.status.idle":"2026-02-06T05:40:29.986447Z","shell.execute_reply.started":"2026-02-06T05:40:25.718588Z","shell.execute_reply":"2026-02-06T05:40:29.985387Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 2: Import libraries\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.models as models\nimport torchvision.transforms as transforms\nimport torchaudio\nimport numpy as np\nimport librosa\nimport soundfile as sf\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import average_precision_score, auc\nfrom torch.utils.data import Dataset, DataLoader\nimport os\nimport random\nfrom typing import Tuple, List, Dict\nfrom collections import defaultdict\n\n# Set random seeds for reproducibility\ntorch.manual_seed(42)\nnp.random.seed(42)\nrandom.seed(42)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-06T05:40:29.988229Z","iopub.execute_input":"2026-02-06T05:40:29.988560Z","iopub.status.idle":"2026-02-06T05:40:39.247328Z","shell.execute_reply.started":"2026-02-06T05:40:29.988521Z","shell.execute_reply":"2026-02-06T05:40:39.246564Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Cell 3: Create Dummy Dataset for Testing\n\nWe'll create a synthetic dataset with paired images and audio","metadata":{}},{"cell_type":"code","source":"class DummySoundSourceDataset(Dataset):\n    \"\"\"Dummy dataset for testing sound source localization\"\"\"\n    \n    def __init__(self, num_samples=500, image_size=(224, 224), audio_length=5, sample_rate=16000):\n        self.num_samples = num_samples\n        self.image_size = image_size\n        self.audio_length = audio_length\n        self.sample_rate = sample_rate\n        self.num_classes = 10  # 10 different sound source categories\n        \n        # Image transforms\n        self.image_transform = transforms.Compose([\n            transforms.Resize(image_size),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        ])\n        \n        print(f\"Created dummy dataset with {num_samples} samples\")\n    \n    def __len__(self):\n        return self.num_samples\n    \n    def generate_synthetic_image(self, idx):\n        \"\"\"Generate synthetic image with a colored region (sound source)\"\"\"\n        np.random.seed(idx)\n        \n        # Create base image\n        image = np.random.randint(50, 100, (*self.image_size, 3), dtype=np.uint8)\n        \n        # Add a colored region representing sound source\n        category = idx % self.num_classes\n        \n        # Define sound source position and size\n        center_y = np.random.randint(50, self.image_size[0] - 50)\n        center_x = np.random.randint(50, self.image_size[1] - 50)\n        size = np.random.randint(30, 80)\n        \n        # Different colors for different categories\n        colors = [\n            [255, 0, 0],    # Red\n            [0, 255, 0],    # Green\n            [0, 0, 255],    # Blue\n            [255, 255, 0],  # Yellow\n            [255, 0, 255],  # Magenta\n            [0, 255, 255],  # Cyan\n            [255, 128, 0],  # Orange\n            [128, 0, 255],  # Purple\n            [0, 128, 255],  # Light Blue\n            [255, 192, 203] # Pink\n        ]\n        \n        color = colors[category]\n        \n        # Draw colored circle\n        y, x = np.ogrid[:self.image_size[0], :self.image_size[1]]\n        mask = (x - center_x)**2 + (y - center_y)**2 <= size**2\n        image[mask] = color\n        \n        # Store ground truth mask\n        gt_mask = mask.astype(np.float32)\n        \n        return Image.fromarray(image), gt_mask, category\n    \n    def generate_synthetic_audio(self, idx, category):\n        \"\"\"Generate synthetic audio signal based on category\"\"\"\n        np.random.seed(idx + 1000)\n        \n        # Base audio\n        samples = int(self.sample_rate * self.audio_length)\n        audio = np.random.randn(samples) * 0.1\n        \n        # Add frequency components based on category\n        base_freq = 200 + category * 100  # Different base frequency per category\n        t = np.linspace(0, self.audio_length, samples)\n        \n        # Add harmonics\n        for harmonic in range(1, 4):\n            freq = base_freq * harmonic\n            audio += 0.3 * np.sin(2 * np.pi * freq * t) / harmonic\n        \n        # Add some noise\n        audio += np.random.randn(samples) * 0.05\n        \n        # Normalize\n        audio = audio / (np.max(np.abs(audio)) + 1e-8)\n        \n        return audio\n    \n    def compute_spectrogram(self, audio):\n        \"\"\"Compute log spectrogram\"\"\"\n        # Compute STFT\n        stft = librosa.stft(audio, n_fft=512, hop_length=160, win_length=400)\n        spectrogram = np.abs(stft)\n        \n        # Convert to log scale\n        log_spec = librosa.amplitude_to_db(spectrogram, ref=np.max)\n        \n        # Normalize\n        log_spec = (log_spec - log_spec.min()) / (log_spec.max() - log_spec.min() + 1e-8)\n        \n        # Resize to fixed size (257 freq bins)\n        target_freq_bins = 257\n        if log_spec.shape[0] < target_freq_bins:\n            # Pad\n            pad_size = target_freq_bins - log_spec.shape[0]\n            log_spec = np.pad(log_spec, ((0, pad_size), (0, 0)), mode='constant')\n        else:\n            log_spec = log_spec[:target_freq_bins, :]\n        \n        return torch.FloatTensor(log_spec).unsqueeze(0)  # Add channel dim\n    \n    def __getitem__(self, idx):\n        image, gt_mask, category = self.generate_synthetic_image(idx)\n        audio = self.generate_synthetic_audio(idx, category)\n        spectrogram = self.compute_spectrogram(audio)\n        \n        # Apply image transforms\n        image_tensor = self.image_transform(image)\n        \n        return {\n            'image': image_tensor,\n            'audio': spectrogram,\n            'gt_mask': torch.FloatTensor(gt_mask),\n            'category': category,\n            'idx': idx\n        }\n\n# Test dataset\ntest_dataset = DummySoundSourceDataset(num_samples=10)\nsample = test_dataset[0]\nprint(f\"Image shape: {sample['image'].shape}\")\nprint(f\"Audio shape: {sample['audio'].shape}\")\nprint(f\"GT mask shape: {sample['gt_mask'].shape}\")\nprint(f\"Category: {sample['category']}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-06T05:40:39.248340Z","iopub.execute_input":"2026-02-06T05:40:39.248790Z","iopub.status.idle":"2026-02-06T05:40:51.258705Z","shell.execute_reply.started":"2026-02-06T05:40:39.248763Z","shell.execute_reply":"2026-02-06T05:40:51.257832Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Cell 4: Encoder Networks\n\nImage and Audio encoders using ResNet-18","metadata":{}},{"cell_type":"code","source":"class ImageEncoder(nn.Module):\n    \"\"\"Image encoder using ResNet-18 with projection layer\"\"\"\n    \n    def __init__(self, output_dim=512):\n        super().__init__()\n        \n        # Load pretrained ResNet-18\n        resnet = models.resnet18(pretrained=True)\n        \n        # Remove the final FC layer and avgpool\n        self.features = nn.Sequential(*list(resnet.children())[:-2])\n        \n        # Add 1x1 conv for projection\n        self.projection = nn.Conv2d(512, output_dim, kernel_size=1)\n        \n    def forward(self, x):\n        # x: (B, 3, H, W)\n        features = self.features(x)  # (B, 512, H/32, W/32)\n        features = self.projection(features)  # (B, output_dim, H/32, W/32)\n        return features\n\n\nclass AudioEncoder(nn.Module):\n    \"\"\"Audio encoder using ResNet-18 adapted for spectrograms\"\"\"\n    \n    def __init__(self, output_dim=512):\n        super().__init__()\n        \n        # Create ResNet-18 for 1-channel input\n        resnet = models.resnet18(pretrained=False)\n        \n        # Modify first conv to accept 1 channel\n        resnet.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        \n        # Remove final FC layer\n        self.features = nn.Sequential(*list(resnet.children())[:-2])\n        \n        # Add max pooling along frequency axis\n        self.freq_pool = nn.AdaptiveMaxPool2d((16, None))\n        \n        # Add 1x1 conv for projection\n        self.projection = nn.Conv2d(512, output_dim, kernel_size=1)\n        \n    def forward(self, x):\n        # x: (B, 1, F, T) where F is frequency bins, T is time\n        features = self.features(x)  # (B, 512, F/32, T/32)\n        features = self.freq_pool(features)  # (B, 512, 16, T/32)\n        features = self.projection(features)  # (B, output_dim, 16, T/32)\n        return features\n\n# Test encoders\nimage_encoder = ImageEncoder(output_dim=512).to(device)\naudio_encoder = AudioEncoder(output_dim=512).to(device)\n\n# Test with dummy data\ndummy_image = torch.randn(2, 3, 224, 224).to(device)\ndummy_audio = torch.randn(2, 1, 257, 160).to(device)\n\nwith torch.no_grad():\n    image_feat = image_encoder(dummy_image)\n    audio_feat = audio_encoder(dummy_audio)\n    \nprint(f\"Image features shape: {image_feat.shape}\")  # Should be (2, 512, 7, 7)\nprint(f\"Audio features shape: {audio_feat.shape}\")  # Should be (2, 512, 16, 5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-06T05:40:51.260708Z","iopub.execute_input":"2026-02-06T05:40:51.261190Z","iopub.status.idle":"2026-02-06T05:40:53.009193Z","shell.execute_reply.started":"2026-02-06T05:40:51.261163Z","shell.execute_reply":"2026-02-06T05:40:53.008154Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Cell 5: Joint Slot Attention Module\n\nThe core module that decomposes features into target and off-target slots","metadata":{}},{"cell_type":"code","source":"class JointSlotAttention(nn.Module):\n    \"\"\"Joint Slot Attention module for feature decomposition\"\"\"\n    \n    def __init__(\n        self,\n        dim=512,\n        num_slots=2,\n        num_iterations=5,\n        hidden_dim=256,\n        eps=1e-8\n    ):\n        super().__init__()\n        \n        self.dim = dim\n        self.num_slots = num_slots\n        self.num_iterations = num_iterations\n        self.eps = eps\n        \n        # Shared initial slots for both modalities\n        self.slots_init = nn.Parameter(torch.randn(1, num_slots, dim) * 0.05)\n        \n        # Layer normalization\n        self.norm_input = nn.LayerNorm(dim)\n        self.norm_slots = nn.LayerNorm(dim)\n        self.norm_pre_ff = nn.LayerNorm(dim)\n        \n        # Linear projections for keys and values\n        self.to_k = nn.Linear(dim, dim, bias=False)\n        self.to_v = nn.Linear(dim, dim, bias=False)\n        \n        # Linear projection for queries (from slots)\n        self.to_q = nn.Linear(dim, dim, bias=False)\n        \n        # GRU for slot updates\n        self.gru = nn.GRUCell(dim, dim)\n        \n        # MLP for slot refinement\n        self.mlp = nn.Sequential(\n            nn.Linear(dim, hidden_dim),\n            nn.ReLU(inplace=True),\n            nn.Linear(hidden_dim, dim)\n        )\n    \n    def forward(self, features):\n        \"\"\"\n        Args:\n            features: (B, C, H, W) or (B, C, F, T) feature maps\n        Returns:\n            slots: (B, num_slots, C) slot representations\n            queries: (B, num_slots, C) query representations\n            attention: (B, num_slots, H*W) or (B, num_slots, F*T) attention maps\n        \"\"\"\n        B = features.shape[0]\n        \n        # Reshape features to (B, N, C) where N = H*W or F*T\n        if features.dim() == 4:\n            B, C, H, W = features.shape\n            features_flat = features.flatten(2).permute(0, 2, 1)  # (B, H*W, C)\n        else:\n            raise ValueError(\"Features must be 4D (B, C, H, W)\")\n        \n        N = features_flat.shape[1]\n        \n        # Normalize and project to keys and values\n        features_norm = self.norm_input(features_flat)  # (B, N, C)\n        k = self.to_k(features_norm)  # (B, N, C)\n        v = self.to_v(features_norm)  # (B, N, C)\n        \n        # Initialize slots\n        slots = self.slots_init.expand(B, -1, -1).contiguous()  # (B, num_slots, C)\n        \n        # Iterative slot attention\n        for _ in range(self.num_iterations):\n            slots_prev = slots\n            \n            # Normalize slots\n            slots_norm = self.norm_slots(slots)  # (B, num_slots, C)\n            \n            # Compute queries\n            q = self.to_q(slots_norm)  # (B, num_slots, C)\n            \n            # Compute attention weights\n            # (B, num_slots, C) @ (B, N, C).T -> (B, num_slots, N)\n            attn_logits = torch.einsum('bnc,bmc->bnm', q, k) / np.sqrt(self.dim)\n            attn = F.softmax(attn_logits, dim=-1)  # (B, num_slots, N)\n            \n            # Weighted mean\n            attn_norm = attn / (attn.sum(dim=-1, keepdim=True) + self.eps)  # (B, num_slots, N)\n            updates = torch.einsum('bnm,bmc->bnc', attn_norm, v)  # (B, num_slots, C)\n            \n            # GRU update\n            slots = self.gru(\n                updates.reshape(-1, self.dim),\n                slots_prev.reshape(-1, self.dim)\n            ).reshape(B, self.num_slots, self.dim)\n            \n            # MLP with residual\n            slots = slots + self.mlp(self.norm_pre_ff(slots))\n        \n        # Compute final queries\n        queries = self.to_q(self.norm_slots(slots))\n        \n        # Compute final attention\n        attn_logits = torch.einsum('bnc,bmc->bnm', queries, k) / np.sqrt(self.dim)\n        final_attn = F.softmax(attn_logits, dim=-1)\n        \n        return slots, queries, final_attn, k, v\n\n# Test Joint Slot Attention\njsa = JointSlotAttention(dim=512, num_slots=2, num_iterations=5).to(device)\n\n# Test with dummy features\ndummy_features = torch.randn(2, 512, 7, 7).to(device)\nslots, queries, attn, keys, values = jsa(dummy_features)\n\nprint(f\"Slots shape: {slots.shape}\")  # (2, 2, 512)\nprint(f\"Queries shape: {queries.shape}\")  # (2, 2, 512)\nprint(f\"Attention shape: {attn.shape}\")  # (2, 2, 49)\nprint(f\"Keys shape: {keys.shape}\")  # (2, 49, 512)\nprint(f\"Values shape: {values.shape}\")  # (2, 49, 512)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-06T05:44:58.284395Z","iopub.execute_input":"2026-02-06T05:44:58.285292Z","iopub.status.idle":"2026-02-06T05:44:58.462146Z","shell.execute_reply.started":"2026-02-06T05:44:58.285259Z","shell.execute_reply":"2026-02-06T05:44:58.461359Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Cell 6: Slot Decoders\n\nReconstruction decoders for slot reconstruction loss","metadata":{}},{"cell_type":"code","source":"class SlotDecoder(nn.Module):\n    \"\"\"MLP decoder for slot reconstruction\"\"\"\n    \n    def __init__(self, slot_dim=512, hidden_dim=1024, output_dim=512):\n        super().__init__()\n        \n        self.mlp = nn.Sequential(\n            nn.Linear(slot_dim, hidden_dim),\n            nn.ReLU(inplace=True),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(inplace=True),\n            nn.Linear(hidden_dim, output_dim)\n        )\n    \n    def forward(self, slots):\n        \"\"\"\n        Args:\n            slots: (B, num_slots, slot_dim)\n        Returns:\n            reconstruction: (B, num_slots, output_dim)\n        \"\"\"\n        return self.mlp(slots)\n\n# Test decoder\ndecoder = SlotDecoder(slot_dim=512, output_dim=512).to(device)\nreconstruction = decoder(slots)\nprint(f\"Reconstruction shape: {reconstruction.shape}\")  # (2, 2, 512)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-06T05:45:04.413041Z","iopub.execute_input":"2026-02-06T05:45:04.413364Z","iopub.status.idle":"2026-02-06T05:45:04.440858Z","shell.execute_reply.started":"2026-02-06T05:45:04.413313Z","shell.execute_reply":"2026-02-06T05:45:04.440152Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Cell 7: Complete Model Architecture\n\nPutting everything together","metadata":{}},{"cell_type":"code","source":"class JointSlotAttentionSSL(nn.Module):\n    \"\"\"Complete Sound Source Localization model with Joint Slot Attention\"\"\"\n    \n    def __init__(\n        self,\n        feature_dim=512,\n        num_slots=2,\n        num_iterations=5,\n        temperature=0.03\n    ):\n        super().__init__()\n        \n        self.feature_dim = feature_dim\n        self.num_slots = num_slots\n        self.temperature = temperature\n        \n        # Encoders\n        self.image_encoder = ImageEncoder(output_dim=feature_dim)\n        self.audio_encoder = AudioEncoder(output_dim=feature_dim)\n        \n        # Joint Slot Attention (shared initial slots)\n        self.slot_attention = JointSlotAttention(\n            dim=feature_dim,\n            num_slots=num_slots,\n            num_iterations=num_iterations\n        )\n        \n        # Decoders for reconstruction loss\n        self.image_decoder = SlotDecoder(slot_dim=feature_dim, output_dim=feature_dim)\n        self.audio_decoder = SlotDecoder(slot_dim=feature_dim, output_dim=feature_dim)\n    \n    def encode(self, images, audios):\n        \"\"\"Encode images and audios to features\"\"\"\n        image_features = self.image_encoder(images)  # (B, C, H, W)\n        audio_features = self.audio_encoder(audios)  # (B, C, F, T)\n        return image_features, audio_features\n    \n    def decompose(self, image_features, audio_features):\n        \"\"\"Decompose features into slots using joint slot attention\"\"\"\n        # Image slot attention\n        image_slots, image_queries, image_attn, image_keys, image_values = self.slot_attention(image_features)\n        \n        # Audio slot attention (shares the same initial slots)\n        audio_slots, audio_queries, audio_attn, audio_keys, audio_values = self.slot_attention(audio_features)\n        \n        return {\n            'image_slots': image_slots,  # (B, 2, C)\n            'image_queries': image_queries,\n            'image_keys': image_keys,\n            'image_values': image_values,\n            'audio_slots': audio_slots,\n            'audio_queries': audio_queries,\n            'audio_keys': audio_keys,\n            'audio_values': audio_values\n        }\n    \n    def forward(self, images, audios):\n        \"\"\"Forward pass\"\"\"\n        # Encode\n        image_features, audio_features = self.encode(images, audios)\n        \n        # Decompose\n        decomposition = self.decompose(image_features, audio_features)\n        \n        # Extract target slots (first slot is target)\n        image_target_slot = decomposition['image_slots'][:, 0]  # (B, C)\n        image_off_target_slot = decomposition['image_slots'][:, 1]\n        audio_target_slot = decomposition['audio_slots'][:, 0]\n        audio_off_target_slot = decomposition['audio_slots'][:, 1]\n        \n        # Store feature map shapes for attention computation\n        B, C, H, W = image_features.shape\n        _, _, F, T = audio_features.shape\n        \n        return {\n            **decomposition,\n            'image_features': image_features,\n            'audio_features': audio_features,\n            'image_target_slot': image_target_slot,\n            'image_off_target_slot': image_off_target_slot,\n            'audio_target_slot': audio_target_slot,\n            'audio_off_target_slot': audio_off_target_slot,\n            'feature_shapes': {'B': B, 'C': C, 'H': H, 'W': W, 'F': F, 'T': T}\n        }\n\n# Test complete model\nmodel = JointSlotAttentionSSL(feature_dim=512, num_slots=2, num_iterations=5).to(device)\n\ndummy_images = torch.randn(2, 3, 224, 224).to(device)\ndummy_audios = torch.randn(2, 1, 257, 160).to(device)\n\nwith torch.no_grad():\n    output = model(dummy_images, dummy_audios)\n\nprint(\"Model output keys:\", list(output.keys()))\nprint(f\"Image target slot shape: {output['image_target_slot'].shape}\")\nprint(f\"Audio target slot shape: {output['audio_target_slot'].shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-06T05:45:08.154727Z","iopub.execute_input":"2026-02-06T05:45:08.155429Z","iopub.status.idle":"2026-02-06T05:45:08.620315Z","shell.execute_reply.started":"2026-02-06T05:45:08.155399Z","shell.execute_reply":"2026-02-06T05:45:08.619664Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Cell 8: Loss Functions\n\nImplementing all four losses: Contrastive, Attention Matching, Slot Divergence, and Reconstruction","metadata":{}},{"cell_type":"code","source":"class SSLoss(nn.Module):\n    \"\"\"Combined loss for Sound Source Localization\"\"\"\n    \n    def __init__(\n        self,\n        temperature=0.03,\n        lambda_match=100.0,\n        lambda_div=0.1,\n        lambda_recon=0.1\n    ):\n        super().__init__()\n        \n        self.temperature = temperature\n        self.lambda_match = lambda_match\n        self.lambda_div = lambda_div\n        self.lambda_recon = lambda_recon\n    \n    def cosine_similarity(self, x, y):\n        \"\"\"Compute cosine similarity with temperature\"\"\"\n        x = F.normalize(x, dim=-1)\n        y = F.normalize(y, dim=-1)\n        return torch.matmul(x, y.T) / self.temperature\n    \n    def contrastive_loss(self, image_target_slots, audio_target_slots, false_negatives_mask=None):\n        \"\"\"\n        InfoNCE contrastive loss between target slots\n        Args:\n            image_target_slots: (B, C)\n            audio_target_slots: (B, C)\n            false_negatives_mask: (B, B) boolean mask where True indicates samples to exclude\n        \"\"\"\n        B = image_target_slots.shape[0]\n        \n        # Compute similarity matrix\n        logits = self.cosine_similarity(image_target_slots, audio_target_slots)  # (B, B)\n        \n        # Labels: diagonal elements are positives\n        labels = torch.arange(B).to(logits.device)\n        \n        # If false negatives mask provided, set those logits to very negative value\n        if false_negatives_mask is not None:\n            logits = logits.masked_fill(false_negatives_mask, -1e9)\n        \n        # InfoNCE loss\n        loss_i2a = F.cross_entropy(logits, labels)\n        loss_a2i = F.cross_entropy(logits.T, labels)\n        \n        return (loss_i2a + loss_a2i) / 2\n    \n    def attention_matching_loss(\n        self,\n        image_queries,\n        audio_queries,\n        image_keys,\n        audio_keys\n    ):\n        \"\"\"\n        Cross-modal attention matching loss\n        Encourages cross-modal attention to match intra-modal attention\n        \"\"\"\n        # Target queries (first slot)\n        image_target_query = image_queries[:, 0]  # (B, C)\n        audio_target_query = audio_queries[:, 0]\n        \n        # Compute intra-modal attention (image -> image)\n        # (B, C) @ (B, N, C).T -> (B, N)\n        intra_image_attn = torch.einsum('bc,bnc->bn', image_target_query, image_keys)\n        intra_image_attn = F.softmax(intra_image_attn / np.sqrt(image_keys.shape[-1]), dim=-1)\n        \n        # Compute cross-modal attention (audio -> image)\n        cross_audio2image_attn = torch.einsum('bc,bnc->bn', audio_target_query, image_keys)\n        cross_audio2image_attn = F.softmax(cross_audio2image_attn / np.sqrt(image_keys.shape[-1]), dim=-1)\n        \n        # Compute intra-modal attention (audio -> audio)\n        intra_audio_attn = torch.einsum('bc,bnc->bn', audio_target_query, audio_keys)\n        intra_audio_attn = F.softmax(intra_audio_attn / np.sqrt(audio_keys.shape[-1]), dim=-1)\n        \n        # Compute cross-modal attention (image -> audio)\n        cross_image2audio_attn = torch.einsum('bc,bnc->bn', image_target_query, audio_keys)\n        cross_image2audio_attn = F.softmax(cross_image2audio_attn / np.sqrt(audio_keys.shape[-1]), dim=-1)\n        \n        # Matching loss with stop-gradient on intra-modal attention\n        loss_match_image = F.mse_loss(cross_audio2image_attn, intra_image_attn.detach())\n        loss_match_audio = F.mse_loss(cross_image2audio_attn, intra_audio_attn.detach())\n        \n        return (loss_match_image + loss_match_audio) / 2\n    \n    def slot_divergence_loss(self, image_slots, audio_slots):\n        \"\"\"\n        Slot divergence loss - encourages slots to be different\n        \"\"\"\n        # Normalize slots\n        image_slots_norm = F.normalize(image_slots, dim=-1)\n        audio_slots_norm = F.normalize(audio_slots, dim=-1)\n        \n        # Compute cosine similarity between target and off-target slots\n        image_sim = torch.einsum('bc,bc->b', image_slots_norm[:, 0], image_slots_norm[:, 1])\n        audio_sim = torch.einsum('bc,bc->b', audio_slots_norm[:, 0], audio_slots_norm[:, 1])\n        \n        # We want similarity to be low (slots should be different)\n        # So we minimize the similarity (maximize divergence)\n        loss_div = (image_sim.mean() + audio_sim.mean()) / 2\n        \n        return loss_div\n    \n    def reconstruction_loss(\n        self,\n        image_slots,\n        audio_slots,\n        image_features,\n        audio_features,\n        image_decoder,\n        audio_decoder\n    ):\n        \"\"\"\n        Slot reconstruction loss\n        \"\"\"\n        # Decode slots\n        image_recon = image_decoder(image_slots)  # (B, 2, C)\n        audio_recon = audio_decoder(audio_slots)\n        \n        # Pool original features to match slot dimension\n        image_features_pooled = image_features.mean(dim=(2, 3))  # (B, C)\n        audio_features_pooled = audio_features.mean(dim=(2, 3))\n        \n        # Reconstruction loss (MSE)\n        # Target slot should reconstruct pooled features\n        loss_recon_image = F.mse_loss(image_recon[:, 0], image_features_pooled)\n        loss_recon_audio = F.mse_loss(audio_recon[:, 0], audio_features_pooled)\n        \n        return (loss_recon_image + loss_recon_audio) / 2\n    \n    def forward(\n        self,\n        model_output,\n        image_decoder,\n        audio_decoder,\n        false_negatives_mask=None\n    ):\n        \"\"\"Compute all losses\"\"\"\n        \n        # Contrastive loss\n        loss_cotr = self.contrastive_loss(\n            model_output['image_target_slot'],\n            model_output['audio_target_slot'],\n            false_negatives_mask\n        )\n        \n        # Attention matching loss\n        loss_match = self.attention_matching_loss(\n            model_output['image_queries'],\n            model_output['audio_queries'],\n            model_output['image_keys'],\n            model_output['audio_keys']\n        )\n        \n        # Slot divergence loss\n        loss_div = self.slot_divergence_loss(\n            model_output['image_slots'],\n            model_output['audio_slots']\n        )\n        \n        # Reconstruction loss\n        loss_recon = self.reconstruction_loss(\n            model_output['image_slots'],\n            model_output['audio_slots'],\n            model_output['image_features'],\n            model_output['audio_features'],\n            image_decoder,\n            audio_decoder\n        )\n        \n        # Total loss\n        total_loss = loss_cotr + self.lambda_match * loss_match + self.lambda_div * loss_div + self.lambda_recon * loss_recon\n\n        \n        return {\n            'total': total_loss,\n            'contrastive': loss_cotr,\n            'matching': loss_match,\n            'divergence': loss_div,\n            'reconstruction': loss_recon\n        }\n\n# Test loss computation\ncriterion = SSLoss(temperature=0.03, lambda_match=100.0, lambda_div=0.1, lambda_recon=0.1)\nlosses = criterion(output, model.image_decoder, model.audio_decoder)\n\nprint(\"Loss components:\")\nfor key, value in losses.items():\n    print(f\"  {key}: {value.item():.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-06T05:47:22.386830Z","iopub.execute_input":"2026-02-06T05:47:22.387558Z","iopub.status.idle":"2026-02-06T05:47:22.500181Z","shell.execute_reply.started":"2026-02-06T05:47:22.387528Z","shell.execute_reply":"2026-02-06T05:47:22.499537Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Cell 9: False Negative Mitigation\n\nImplementation of k-reciprocal nearest neighbors","metadata":{}},{"cell_type":"code","source":"def compute_false_negatives(image_target_slots, audio_target_slots, k=20):\n    \"\"\"\n    Compute false negatives using k-reciprocal nearest neighbors\n    \n    Args:\n        image_target_slots: (B, C)\n        audio_target_slots: (B, C)\n        k: number of nearest neighbors\n    \n    Returns:\n        false_negatives_mask: (B, B) boolean mask where True indicates false negatives\n    \"\"\"\n    B = image_target_slots.shape[0]\n    device = image_target_slots.device\n    \n    # Clamp k to be at most B-1 (can't have more neighbors than other samples)\n    k = min(k, B - 1)\n    \n    if k <= 0:\n        # If batch size is 1, no false negatives possible\n        return torch.zeros(B, B, dtype=torch.bool, device=device)\n    \n    with torch.no_grad():\n        # Normalize features\n        image_norm = F.normalize(image_target_slots, dim=-1)\n        audio_norm = F.normalize(audio_target_slots, dim=-1)\n        \n        # Compute similarity matrices\n        image_sim = torch.matmul(image_norm, image_norm.T)  # (B, B)\n        audio_sim = torch.matmul(audio_norm, audio_norm.T)  # (B, B)\n        \n        # Get k-nearest neighbors for each sample\n        # k+1 because sample itself is included, but we clamp to B\n        k_actual = min(k + 1, B)\n        _, image_nn = torch.topk(image_sim, k_actual, dim=-1)\n        _, audio_nn = torch.topk(audio_sim, k_actual, dim=-1)\n        \n        # Remove self from nearest neighbors\n        image_nn = image_nn[:, 1:]  # (B, k)\n        audio_nn = audio_nn[:, 1:]\n        \n        # Check reciprocal relationship\n        false_negatives_mask = torch.zeros(B, B, dtype=torch.bool, device=device)\n        \n        for i in range(B):\n            for j in image_nn[i]:\n                j = j.item()\n                if i in image_nn[j]:\n                    # Check if also reciprocal in audio\n                    if j in audio_nn[i] and i in audio_nn[j]:\n                        false_negatives_mask[i, j] = True\n                        false_negatives_mask[j, i] = True\n    \n    # Don't mask diagonal (positives should remain)\n    false_negatives_mask.fill_diagonal_(False)\n    \n    return false_negatives_mask\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-06T05:48:39.275149Z","iopub.execute_input":"2026-02-06T05:48:39.276026Z","iopub.status.idle":"2026-02-06T05:48:39.284108Z","shell.execute_reply.started":"2026-02-06T05:48:39.275995Z","shell.execute_reply":"2026-02-06T05:48:39.283404Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Cell 10: Inference and Localization\n\nGenerate sound source localization maps with IQR refinement","metadata":{}},{"cell_type":"code","source":"class InferenceModule:\n    \"\"\"Inference module for sound source localization\"\"\"\n    \n    def __init__(self, model, threshold=0.5, alpha=0.6):\n        self.model = model\n        self.threshold = threshold\n        self.alpha = alpha  # Balance parameter for IQR\n        self.model.eval()\n    \n    @torch.no_grad()\n    def localize(self, images, audios, return_raw=False):\n        \"\"\"\n        Perform sound source localization\n        \n        Args:\n            images: (B, 3, H, W)\n            audios: (B, 1, F, T)\n            return_raw: if True, return raw attention maps\n        \n        Returns:\n            localization_maps: (B, H, W) binary or soft localization maps\n        \"\"\"\n        # Forward pass\n        output = self.model(images, audios)\n        \n        # Get feature shapes\n        B, C, H, W = output['image_features'].shape\n        \n        # Get target query and image keys\n        audio_target_query = output['audio_queries'][:, 0]  # (B, C)\n        image_target_query = output['image_queries'][:, 0]\n        image_keys = output['image_keys']  # (B, H*W, C)\n        \n        # Compute cross-modal attention (audio target -> image)\n        cross_attn = torch.einsum('bc,bnc->bn', audio_target_query, image_keys)\n        cross_attn = cross_attn / np.sqrt(C)\n        cross_attn = F.softmax(cross_attn, dim=-1)  # (B, H*W)\n        \n        # Reshape to image dimensions\n        cross_attn_map = cross_attn.view(B, H, W)  # (B, H, W)\n        \n        # Compute intra-modal attention for IQR (image target -> image)\n        intra_attn = torch.einsum('bc,bnc->bn', image_target_query, image_keys)\n        intra_attn = intra_attn / np.sqrt(C)\n        intra_attn = F.softmax(intra_attn, dim=-1)\n        intra_attn_map = intra_attn.view(B, H, W)\n        \n        # Image-Query based Refinement (IQR)\n        refined_map = self.alpha * cross_attn_map + (1 - self.alpha) * intra_attn_map\n        \n        if return_raw:\n            return {\n                'cross_modal_attention': cross_attn_map,\n                'intra_modal_attention': intra_attn_map,\n                'refined_attention': refined_map,\n                'binary_mask': (refined_map > self.threshold).float()\n            }\n        \n        # Upsample to original image size (224x224)\n        refined_map_upsampled = F.interpolate(\n            refined_map.unsqueeze(1),\n            size=(224, 224),\n            mode='bilinear',\n            align_corners=False\n        ).squeeze(1)\n        \n        # Threshold to get binary mask\n        binary_mask = (refined_map_upsampled > self.threshold).float()\n        \n        return refined_map_upsampled, binary_mask\n\n# Test inference\ninference = InferenceModule(model, threshold=0.5, alpha=0.6)\nloc_map, binary_mask = inference.localize(dummy_images, dummy_audios)\n\nprint(f\"Localization map shape: {loc_map.shape}\")\nprint(f\"Binary mask shape: {binary_mask.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-06T05:48:44.763890Z","iopub.execute_input":"2026-02-06T05:48:44.764199Z","iopub.status.idle":"2026-02-06T05:48:44.822724Z","shell.execute_reply.started":"2026-02-06T05:48:44.764175Z","shell.execute_reply":"2026-02-06T05:48:44.822004Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Cell 11: Training Loop","metadata":{}},{"cell_type":"code","source":"def train_epoch(model, dataloader, criterion, optimizer, device, epoch, use_false_negative_mitigation=True, k_neighbors=20):\n    \"\"\"Train for one epoch\"\"\"\n    model.train()\n    total_losses = defaultdict(float)\n    num_batches = 0\n    \n    for batch_idx, batch in enumerate(dataloader):\n        images = batch['image'].to(device)\n        audios = batch['audio'].to(device)\n        \n        # Forward pass\n        output = model(images, audios)\n        \n        # Compute false negatives if enabled\n        false_negatives_mask = None\n        if use_false_negative_mitigation and batch_idx % 5 == 0:  # Compute every 5 batches for efficiency\n            false_negatives_mask = compute_false_negatives(\n                output['image_target_slot'],\n                output['audio_target_slot'],\n                k=k_neighbors\n            )\n        \n        # Compute losses\n        losses = criterion(output, model.image_decoder, model.audio_decoder, false_negatives_mask)\n        \n        # Backward pass\n        optimizer.zero_grad()\n        losses['total'].backward()\n        optimizer.step()\n        \n        # Accumulate losses\n        for key, value in losses.items():\n            total_losses[key] += value.item()\n        \n        num_batches += 1\n        \n        if batch_idx % 10 == 0:\n            print(f\"Epoch {epoch}, Batch {batch_idx}/{len(dataloader)}, \"\n                  f\"Total Loss: {losses['total'].item():.4f}\")\n    \n    # Average losses\n    avg_losses = {k: v / num_batches for k, v in total_losses.items()}\n    return avg_losses\n\n\n@torch.no_grad()\ndef evaluate(model, dataloader, device):\n    \"\"\"Evaluate the model\"\"\"\n    model.eval()\n    inference = InferenceModule(model, threshold=0.5, alpha=0.6)\n    \n    all_preds = []\n    all_gts = []\n    \n    for batch in dataloader:\n        images = batch['image'].to(device)\n        audios = batch['audio'].to(device)\n        gt_masks = batch['gt_mask'].numpy()\n        \n        # Get localization maps\n        loc_maps, binary_masks = inference.localize(images, audios)\n        \n        # Store predictions and ground truths\n        loc_maps_np = loc_maps.cpu().numpy()\n        \n        for pred, gt in zip(loc_maps_np, gt_masks):\n            all_preds.append(pred.flatten())\n            all_gts.append(gt.flatten())\n    \n    # Compute metrics\n    all_preds = np.concatenate(all_preds)\n    all_gts = np.concatenate(all_gts)\n    \n    # Average Precision\n    ap = average_precision_score(all_gts, all_preds)\n    \n    # cIoU at threshold 0.5\n    binary_preds = (all_preds > 0.5).astype(float)\n    intersection = np.sum((binary_preds == 1) & (all_gts == 1))\n    union = np.sum((binary_preds == 1) | (all_gts == 1))\n    ciou = intersection / (union + 1e-8)\n    \n    return {'AP': ap, 'cIoU': ciou}\n\nprint(\"Training functions defined successfully\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-06T05:48:51.550214Z","iopub.execute_input":"2026-02-06T05:48:51.550534Z","iopub.status.idle":"2026-02-06T05:48:51.561677Z","shell.execute_reply.started":"2026-02-06T05:48:51.550510Z","shell.execute_reply":"2026-02-06T05:48:51.560764Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Cell 12: Main Training Execution","metadata":{}},{"cell_type":"code","source":"# Hyperparameters (from paper)\nCONFIG = {\n    'feature_dim': 512,\n    'num_slots': 2,\n    'num_iterations': 5,\n    'temperature': 0.03,\n    'lambda_match': 100.0,\n    'lambda_div': 0.1,\n    'lambda_recon': 0.1,\n    'learning_rate': 5e-5,\n    'weight_decay': 1e-2,\n    'batch_size': 16,  # Reduced for Kaggle GPU\n    'num_epochs': 5,   # Reduced for quick testing\n    'k_neighbors': 20,\n    'threshold': 0.5,\n    'alpha': 0.6\n}\n\nprint(\"Configuration:\")\nfor k, v in CONFIG.items():\n    print(f\"  {k}: {v}\")\n\n# Create datasets\ntrain_dataset = DummySoundSourceDataset(num_samples=500)\nval_dataset = DummySoundSourceDataset(num_samples=100)\n\n# Create dataloaders\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=CONFIG['batch_size'],\n    shuffle=True,\n    num_workers=0,  # Set to 0 for Kaggle\n    pin_memory=True if torch.cuda.is_available() else False\n)\n\nval_loader = DataLoader(\n    val_dataset,\n    batch_size=CONFIG['batch_size'],\n    shuffle=False,\n    num_workers=0\n)\n\nprint(f\"\\nTrain dataset size: {len(train_dataset)}\")\nprint(f\"Validation dataset size: {len(val_dataset)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-06T05:48:59.649969Z","iopub.execute_input":"2026-02-06T05:48:59.650705Z","iopub.status.idle":"2026-02-06T05:48:59.657192Z","shell.execute_reply.started":"2026-02-06T05:48:59.650670Z","shell.execute_reply":"2026-02-06T05:48:59.656593Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize model\nmodel = JointSlotAttentionSSL(\n    feature_dim=CONFIG['feature_dim'],\n    num_slots=CONFIG['num_slots'],\n    num_iterations=CONFIG['num_iterations'],\n    temperature=CONFIG['temperature']\n).to(device)\n\n# Initialize loss\ncriterion = SSLoss(\n    temperature=CONFIG['temperature'],\n    lambda_match=CONFIG['lambda_match'],\n    lambda_div=CONFIG['lambda_div'],\n    lambda_recon=CONFIG['lambda_recon']\n)\n\n# Initialize optimizer\noptimizer = torch.optim.AdamW(\n    model.parameters(),\n    lr=CONFIG['learning_rate'],\n    weight_decay=CONFIG['weight_decay']\n)\n\nprint(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\nprint(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-06T05:49:06.210711Z","iopub.execute_input":"2026-02-06T05:49:06.211148Z","iopub.status.idle":"2026-02-06T05:49:06.665653Z","shell.execute_reply.started":"2026-02-06T05:49:06.211107Z","shell.execute_reply":"2026-02-06T05:49:06.664970Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Training loop\nprint(\"\\n\" + \"=\"*50)\nprint(\"Starting Training\")\nprint(\"=\"*50 + \"\\n\")\n\nhistory = {\n    'train_loss': [],\n    'val_metrics': []\n}\n\nfor epoch in range(1, CONFIG['num_epochs'] + 1):\n    print(f\"\\nEpoch {epoch}/{CONFIG['num_epochs']}\")\n    print(\"-\" * 30)\n    \n    # Train\n    train_losses = train_epoch(\n        model,\n        train_loader,\n        criterion,\n        optimizer,\n        device,\n        epoch,\n        use_false_negative_mitigation=True,\n        k_neighbors=CONFIG['k_neighbors']\n    )\n    \n    print(f\"\\nTrain Losses - Total: {train_losses['total']:.4f}, \"\n          f\"Contr: {train_losses['contrastive']:.4f}, \"\n          f\"Match: {train_losses['matching']:.4f}\")\n    \n    history['train_loss'].append(train_losses)\n    \n    # Evaluate\n    val_metrics = evaluate(model, val_loader, device)\n    print(f\"Validation - AP: {val_metrics['AP']:.4f}, cIoU: {val_metrics['cIoU']:.4f}\")\n    \n    history['val_metrics'].append(val_metrics)\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"Training Complete!\")\nprint(\"=\"*50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-06T05:49:12.687246Z","iopub.execute_input":"2026-02-06T05:49:12.687775Z","iopub.status.idle":"2026-02-06T05:50:40.891277Z","shell.execute_reply.started":"2026-02-06T05:49:12.687745Z","shell.execute_reply":"2026-02-06T05:50:40.890496Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Cell 13: Visualization","metadata":{}},{"cell_type":"code","source":"def visualize_results(model, dataset, num_samples=4):\n    \"\"\"Visualize sound source localization results\"\"\"\n    model.eval()\n    inference = InferenceModule(model, threshold=0.5, alpha=0.6)\n    \n    fig, axes = plt.subplots(num_samples, 4, figsize=(16, 4*num_samples))\n    \n    for i in range(num_samples):\n        # Get sample\n        sample = dataset[i]\n        image = sample['image'].unsqueeze(0).to(device)\n        audio = sample['audio'].unsqueeze(0).to(device)\n        gt_mask = sample['gt_mask'].numpy()\n        \n        # Denormalize image for visualization\n        img_vis = sample['image'].permute(1, 2, 0).cpu().numpy()\n        img_vis = img_vis * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n        img_vis = np.clip(img_vis, 0, 1)\n        \n        # Get localization\n        with torch.no_grad():\n            results = inference.localize(image, audio, return_raw=True)\n            refined_map = results['refined_attention'].squeeze().cpu().numpy()\n            binary_mask = results['binary_mask'].squeeze().cpu().numpy()\n        \n        # Upsample to match image size\n        refined_map_up = F.interpolate(\n            torch.FloatTensor(refined_map).unsqueeze(0).unsqueeze(0),\n            size=(224, 224),\n            mode='bilinear',\n            align_corners=False\n        ).squeeze().numpy()\n        \n        # Plot\n        axes[i, 0].imshow(img_vis)\n        axes[i, 0].set_title('Original Image')\n        axes[i, 0].axis('off')\n        \n        axes[i, 1].imshow(gt_mask, cmap='jet')\n        axes[i, 1].set_title('Ground Truth')\n        axes[i, 1].axis('off')\n        \n        axes[i, 2].imshow(refined_map_up, cmap='jet')\n        axes[i, 2].set_title('Predicted Heatmap')\n        axes[i, 2].axis('off')\n        \n        # Overlay\n        axes[i, 3].imshow(img_vis)\n        axes[i, 3].imshow(refined_map_up, cmap='jet', alpha=0.5)\n        axes[i, 3].set_title('Overlay')\n        axes[i, 3].axis('off')\n    \n    plt.tight_layout()\n    plt.savefig('ssl_results.png', dpi=150, bbox_inches='tight')\n    plt.show()\n    print(\"Results saved to ssl_results.png\")\n\n# Visualize results\nvisualize_results(model, val_dataset, num_samples=4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-06T05:50:46.832221Z","iopub.execute_input":"2026-02-06T05:50:46.832524Z","iopub.status.idle":"2026-02-06T05:50:51.735422Z","shell.execute_reply.started":"2026-02-06T05:50:46.832499Z","shell.execute_reply":"2026-02-06T05:50:51.734377Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot training history\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Loss curves\nepochs = range(1, len(history['train_loss']) + 1)\naxes[0].plot(epochs, [l['total'] for l in history['train_loss']], label='Total', marker='o')\naxes[0].plot(epochs, [l['contrastive'] for l in history['train_loss']], label='Contrastive', marker='s')\naxes[0].plot(epochs, [l['matching'] for l in history['train_loss']], label='Matching', marker='^')\naxes[0].set_xlabel('Epoch')\naxes[0].set_ylabel('Loss')\naxes[0].set_title('Training Losses')\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\n# Validation metrics\naxes[1].plot(epochs, [m['AP'] for m in history['val_metrics']], label='AP', marker='o')\naxes[1].plot(epochs, [m['cIoU'] for m in history['val_metrics']], label='cIoU', marker='s')\naxes[1].set_xlabel('Epoch')\naxes[1].set_ylabel('Metric Value')\naxes[1].set_title('Validation Metrics')\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.savefig('training_history.png', dpi=150, bbox_inches='tight')\nplt.show()\nprint(\"Training history saved to training_history.png\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-06T05:51:13.896491Z","iopub.execute_input":"2026-02-06T05:51:13.897169Z","iopub.status.idle":"2026-02-06T05:51:14.649570Z","shell.execute_reply.started":"2026-02-06T05:51:13.897122Z","shell.execute_reply":"2026-02-06T05:51:14.648869Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Cell 14: Cross-Modal Retrieval Demo\n\nDemonstrate the model's ability to retrieve matching audio given an image and vice versa","metadata":{}},{"cell_type":"code","source":"def cross_modal_retrieval(model, dataset, num_queries=3, top_k=3):\n    \"\"\"Demonstrate cross-modal retrieval\"\"\"\n    model.eval()\n    \n    # Extract all features\n    all_image_slots = []\n    all_audio_slots = []\n    \n    dataloader = DataLoader(dataset, batch_size=32, shuffle=False)\n    \n    with torch.no_grad():\n        for batch in dataloader:\n            images = batch['image'].to(device)\n            audios = batch['audio'].to(device)\n            \n            output = model(images, audios)\n            all_image_slots.append(output['image_target_slot'].cpu())\n            all_audio_slots.append(output['audio_target_slot'].cpu())\n    \n    all_image_slots = torch.cat(all_image_slots, dim=0)  # (N, C)\n    all_audio_slots = torch.cat(all_audio_slots, dim=0)  # (N, C)\n    \n    # Normalize\n    image_norm = F.normalize(all_image_slots, dim=-1)\n    audio_norm = F.normalize(all_audio_slots, dim=-1)\n    \n    # Compute similarity matrix\n    sim_matrix = torch.matmul(image_norm, audio_norm.T)  # (N, N)\n    \n    # Visualize retrieval results\n    fig, axes = plt.subplots(num_queries, top_k + 1, figsize=(3*(top_k+1), 3*num_queries))\n    \n    for q in range(num_queries):\n        query_idx = q * (len(dataset) // num_queries)\n        query_sample = dataset[query_idx]\n        \n        # Show query image\n        img_vis = query_sample['image'].permute(1, 2, 0).cpu().numpy()\n        img_vis = img_vis * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n        img_vis = np.clip(img_vis, 0, 1)\n        \n        axes[q, 0].imshow(img_vis)\n        axes[q, 0].set_title(f'Query (Cat: {query_sample[\"category\"]})')\n        axes[q, 0].axis('off')\n        \n        # Get top-k retrieved audio indices\n        similarities = sim_matrix[query_idx]\n        top_indices = torch.topk(similarities, top_k).indices\n        \n        # Show retrieved images (corresponding to top audio matches)\n        for k, idx in enumerate(top_indices):\n            retrieved_sample = dataset[idx.item()]\n            img_vis_k = retrieved_sample['image'].permute(1, 2, 0).cpu().numpy()\n            img_vis_k = img_vis_k * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n            img_vis_k = np.clip(img_vis_k, 0, 1)\n            \n            match_status = \"\" if retrieved_sample['category'] == query_sample['category'] else \"\"\n            axes[q, k+1].imshow(img_vis_k)\n            axes[q, k+1].set_title(f'Rank {k+1} {match_status}\\n(Cat: {retrieved_sample[\"category\"]})')\n            axes[q, k+1].axis('off')\n    \n    plt.tight_layout()\n    plt.savefig('cross_modal_retrieval.png', dpi=150, bbox_inches='tight')\n    plt.show()\n    print(\"Cross-modal retrieval results saved to cross_modal_retrieval.png\")\n\n# Run retrieval demo\ncross_modal_retrieval(model, val_dataset, num_queries=3, top_k=3)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-06T05:52:59.152592Z","iopub.execute_input":"2026-02-06T05:52:59.153315Z","iopub.status.idle":"2026-02-06T05:53:03.299195Z","shell.execute_reply.started":"2026-02-06T05:52:59.153284Z","shell.execute_reply":"2026-02-06T05:53:03.297235Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"*The cross-modal retrieval visualization gives an intuitive view of how the model connects what it sees with what it hears. Given an input image, the model focuses on the main object that is likely to produce sound and uses this representation to search for similar audio examples.\n\nWhen the retrieved sounds match the visual object, it suggests that the model has learned a meaningful audiovisual association at the object level. Mismatches are also informative, as they often show that the model is responding to shared acoustic characteristics rather than true semantic correspondence. Overall, the visualization supports the idea that separating target and off-target slots helps isolate object-specific information while reducing the influence of background content.*","metadata":{}},{"cell_type":"markdown","source":"## Cell 15: Save Model","metadata":{}},{"cell_type":"code","source":"# Save model checkpoint\ncheckpoint = {\n    'model_state_dict': model.state_dict(),\n    'optimizer_state_dict': optimizer.state_dict(),\n    'config': CONFIG,\n    'history': history\n}\n\ntorch.save(checkpoint, 'joint_slot_attention_ssl.pth')\nprint(\"Model saved to joint_slot_attention_ssl.pth\")\n\n# Print final summary\nprint(\"\\n\" + \"=\"*50)\nprint(\"FINAL SUMMARY\")\nprint(\"=\"*50)\nprint(f\"Final Validation AP: {history['val_metrics'][-1]['AP']:.4f}\")\nprint(f\"Final Validation cIoU: {history['val_metrics'][-1]['cIoU']:.4f}\")\nprint(f\"Total Training Epochs: {CONFIG['num_epochs']}\")\nprint(\"=\"*50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-06T05:53:17.811833Z","iopub.execute_input":"2026-02-06T05:53:17.812388Z","iopub.status.idle":"2026-02-06T05:53:18.580635Z","shell.execute_reply.started":"2026-02-06T05:53:17.812358Z","shell.execute_reply":"2026-02-06T05:53:18.579959Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Cell 16: Ablation Study \n\nTest the impact of each loss component","metadata":{}},{"cell_type":"code","source":"def run_ablation_study(base_config, ablation_configs, train_dataset, val_dataset, device):\n    \"\"\"Run ablation study by training with different loss configurations\"\"\"\n    results = {}\n    \n    for config_name, config_changes in ablation_configs.items():\n        print(f\"\\n{'='*50}\")\n        print(f\"Running: {config_name}\")\n        print(f\"{'='*50}\")\n        \n        # Merge configs\n        config = {**base_config, **config_changes}\n        \n        # Create model\n        model = JointSlotAttentionSSL(\n            feature_dim=config['feature_dim'],\n            num_slots=config['num_slots'],\n            num_iterations=config['num_iterations']\n        ).to(device)\n        \n        # Create loss with ablation\n        criterion = SSLoss(\n            temperature=config['temperature'],\n            lambda_match=config.get('lambda_match', 100.0),\n            lambda_div=config.get('lambda_div', 0.1),\n            lambda_recon=config.get('lambda_recon', 0.1)\n        )\n        \n        optimizer = torch.optim.AdamW(\n            model.parameters(),\n            lr=config['learning_rate'],\n            weight_decay=config['weight_decay']\n        )\n        \n        # Train for fewer epochs for quick ablation\n        num_epochs = 2\n        train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n        val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False)\n        \n        for epoch in range(1, num_epochs + 1):\n            train_losses = train_epoch(model, train_loader, criterion, optimizer, device, epoch)\n            print(f\"Epoch {epoch}: Loss = {train_losses['total']:.4f}\")\n        \n        # Evaluate\n        val_metrics = evaluate(model, val_loader, device)\n        results[config_name] = val_metrics\n        print(f\"Final - AP: {val_metrics['AP']:.4f}, cIoU: {val_metrics['cIoU']:.4f}\")\n    \n    return results\n\n# Define ablation configurations\nablation_configs = {\n    'Full Model': {},\n    'No Matching Loss': {'lambda_match': 0.0},\n    'No Divergence Loss': {'lambda_div': 0.0},\n    'No Reconstruction Loss': {'lambda_recon': 0.1},\n    'Contrastive Only': {'lambda_match': 0.0, 'lambda_div': 0.0, 'lambda_recon': 0.0}\n}\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-06T05:53:23.897746Z","iopub.execute_input":"2026-02-06T05:53:23.898488Z","iopub.status.idle":"2026-02-06T05:53:23.907124Z","shell.execute_reply.started":"2026-02-06T05:53:23.898455Z","shell.execute_reply":"2026-02-06T05:53:23.906262Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# run ablation study \nablation_results = run_ablation_study(CONFIG, ablation_configs, train_dataset, val_dataset, device)\n\n# Print results\nprint(\"\\n\" + \"=\"*50)\nprint(\"ABLATION STUDY RESULTS\")\nprint(\"=\"*50)\nfor name, metrics in ablation_results.items():\n     print(f\"{name:25s} - AP: {metrics['AP']:.4f}, cIoU: {metrics['cIoU']:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-06T05:54:08.093146Z","iopub.execute_input":"2026-02-06T05:54:08.093482Z","iopub.status.idle":"2026-02-06T05:56:52.631957Z","shell.execute_reply.started":"2026-02-06T05:54:08.093456Z","shell.execute_reply":"2026-02-06T05:56:52.631173Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Even with the simplified dataset and limited training, the ablation study shows consistent patterns. All model variants report zero cIoU, suggesting that the model has not yet learned precise spatial localization. This outcome is more likely due to coarse heatmaps, strict evaluation thresholds, and insufficient training, rather than a complete lack of learning.\n\nAt the same time, the variation in AP across configurations indicates that the model does capture meaningful audiovisual relationships at a coarse level. The full model performs best, supporting the idea that the losses work better in combination. The most noticeable drop occurs when the matching loss is removed, underscoring its role in cross-modal alignment. The divergence loss provides additional improvement by encouraging distinct slot representations, while the reconstruction loss appears to act mainly as a mild regularizer.> ","metadata":{}}]}